#!/usr/bin/env python
import sys
import psycopg2
import datetime
import locale
import string
import time
import argparse
import urllib2
import threading
from random import shuffle
from datetime import timedelta

sys.path.append ('googleplay_api')
from googleplay import GooglePlayAPI, LoginError

def enumchar (level, aset):
    if level < 2: return aset
    return [val + newval for newval in aset for val in enumchar(level-1,aset)]

class GPCrawl (threading.Thread):

    def __init__(self, androidid, dbname, user, args, threadid):

        # Initialize threading
        threading.Thread.__init__(self)

        self.threadid = threadid
        self.refresh_token_after = args.refresh
        self.args = args
        self.verbose = args.verbose

        self.num_unsuccessful = 0

        # Set locale such that parsing of download numbers works. Note, that
        # this must be changed consistently with the lang value below.
        locale.setlocale(locale.LC_ALL, 'en_US.utf8')

        self.api = GooglePlayAPI(androidId = androidid, lang = 'en_us')

        self.conn = psycopg2.connect('dbname=%s user=%s' % (dbname, user))
        self.cur = self.conn.cursor()

        if args.destroy:
            self.cur.execute ('DROP TABLE apps');

        # Create database:
        #   sudo -u postgres createdb google_play_test
        table = '''
            CREATE TABLE IF NOT EXISTS apps
            (
                search                              TEXT,
                docid                               TEXT,
                title                               TEXT,
                creator                             TEXT,
                offer_micros                        INTEGER,
                offer_currencyCode                  TEXT,
                offer_formatedAmount                TEXT,
                offer_checkoutFlowRequired          BOOLEAN,
                offer_offerType                     INTEGER,
                details_appDetails_versionCode      INTEGER,
                details_appDetails_installationSize INTEGER,
                details_appDetails_numDownloads     BIGINT,
                details_appDetails_packageName      TEXT,
                details_appDetails_uploadDate       TEXT,
                details_appDetails_file_fileType    INTEGER,
                details_appDetails_file_versionCode INTEGER,
                details_appDetails_file_size        BIGINT,
                aggregateRating_type                INTEGER,
                aggregateRating_starRating          REAL,
                aggregateRating_ratingsCount        INTEGER,
                aggregateRating_oneStarRating       INTEGER,
                aggregateRating_twoStarRating       INTEGER,
                aggregateRating_threeStarRating     INTEGER,
                aggregateRating_fourStarRating      INTEGER,
                aggregateRating_fiveStarRating      INTEGER,
                aggregateRating_commentCount        INTEGER,
                detailsUrl                          TEXT,
                sharesUrl                           TEXT,
                detailsReusable                     BOOLEAN,
                duplicates                          INTEGER,
                crawlDate                           TIMESTAMP,
                PRIMARY KEY(details_appDetails_packageName, details_appDetails_versionCode)
            )
        '''
        self.cur.execute(table)

        cache = '''
            CREATE TABLE IF NOT EXISTS cache
            (
                search      TEXT,
                requests    INTEGER,
                PRIMARY KEY(search)
            )
        '''
        self.cur.execute(cache)

        self.conn.commit()

    def login(self, fetch = False):
        if fetch:
            if self.verbose:
                print ("[%s]: Fetching token from '%s'" % (self.threadid, self.args.token))
            self.token = urllib2.urlopen(self.args.token).read()
        if self.verbose:
            print ("[%s]: Logging in with token '%s'" % (self.threadid, self.token))
        self.api.login(None, None, self.token)

    def mark_cached(self, searchterm):

        querystring = '''
            INSERT INTO cache VALUES (%s,%s)
                ON CONFLICT (search)
                    DO UPDATE SET requests = cache.requests + 1
        '''
        self.cur.execute(querystring, (searchterm,1))
        self.conn.commit()

        if self.verbose:
            print("[%s]: Marked '%s' as cached" % (self.threadid, searchterm))

    def cached(self, searchterm):

        self.cur.execute('SELECT requests FROM cache WHERE search LIKE %s', (searchterm,))
        result = self.cur.fetchall()
        if result:
            return result[0][0]

        # Not found
        return 0

    def unique_ids(self):

        self.conn.commit()
        self.cur.execute('SELECT count(*) FROM apps')
        result = self.cur.fetchall()
        return result[0][0]

    def check_terminate(self, new):

        if not self.args.terminate:
            return

        if (new > 0):
            self.num_unsuccessful = 0
        else:
            self.num_unsuccessful += 1

        if self.num_unsuccessful > self.args.terminate:
            print ("[%s]: %d unsuccessful attempts to get new app, terminating" % (self.threadid, self.num_unsuccessful,))
            sys.exit(0)

    def search(self, query):

        num_results = 0
        sleeptime = 15

        num_cached = self.cached (query)
        if  num_cached >= self.args.cache:
            if self.verbose:
                print ("[%s]: Cached %d times, not performing query for '%s'" % (self.threadid, num_cached, query))
            self.check_terminate(0)
            return 0

        uids_before = self.unique_ids()

        while True:
            try:
                results = self.api.search (query, nb_results=200)
                break
            except KeyboardInterrupt: raise
            except Exception as e:
                print ("[%s]: Query error (%s), sleeping for %d seconds" % (self.threadid, str(e), sleeptime))
                time.sleep (sleeptime)
                self.login(self.refresh_token_after <= sleeptime)
                sleeptime *= 2

        for docs in results.doc:
            for result in docs.child:

                num_results += 1

                ad = result.details.appDetails
                of = result.offer[0]

                upload_date   = datetime.datetime.strptime(ad.uploadDate, "%b %d, %Y")
                num_downloads = locale.atoi(ad.numDownloads.rstrip('+'))
                querystring = '''
                    INSERT INTO apps VALUES
                        (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,
                         %s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,now())
                    ON CONFLICT (details_appDetails_packageName, details_appDetails_versionCode)
                        DO UPDATE
                            SET duplicates = apps.duplicates + 1, crawlDate = now()
                '''

                querydata = \
                    (query,
                     result.docid,
                     result.title,
                     result.creator,
                     of.micros,
                     of.currencyCode,
                     of.formattedAmount,
                     of.checkoutFlowRequired,
                     of.offerType,
                     ad.versionCode,
                     ad.installationSize,
                     num_downloads,
                     ad.packageName,
                     upload_date.date(),
                     ad.file[0].fileType,
                     ad.file[0].versionCode,
                     ad.file[0].size,
                     result.aggregateRating.type,
                     result.aggregateRating.starRating,
                     result.aggregateRating.ratingsCount,
                     result.aggregateRating.oneStarRatings,
                     result.aggregateRating.twoStarRatings,
                     result.aggregateRating.threeStarRatings,
                     result.aggregateRating.fourStarRatings,
                     result.aggregateRating.fiveStarRatings,
                     result.aggregateRating.commentCount,
                     result.detailsUrl,
                     result.shareUrl,
                     result.detailsReusable,
                     0)

                try:
                    self.cur.execute(querystring, querydata)
                except psycopg2.DataError as e:
                    print ("[%s]: Data error: %s" % (self.threadid, str(e)))
                    print ("[%s]: " % (self.threadid, str(querydata)))

        self.conn.commit()
        self.mark_cached(query)

        uids_delta = self.unique_ids() - uids_before
        self.check_terminate(uids_delta)

        if self.verbose:
            print ("[%s]: Searched '%s', got %d results, %d new packages" % (self.threadid, query, num_results, uids_delta))

        return uids_delta

    def search_list(self, queries):

        query_num = 0
        starttime = time.time()
        for query in queries:
            self.search(query)
            query_num += 1
            if self.verbose:
                print ("[%s]: Searching %d/%d: '%s'" % (self.threadid, query_num, len(queries), query))
            self.search(query)
            duration = time.time() - starttime
            if self.verbose:
                time_to_go = (len(queries) - query_num) * query_num / duration
                print ("[%s]: Duration %s seconds, %d unique packages, ETA=%s" %
                    (self.threadid, timedelta(seconds=duration), self.unique_ids(), timedelta(seconds=time_to_go)))

    def seed(self, length):
        for run in (1,2):
            self.search_list(enumchar(length, string.ascii_lowercase))

    def creator(self):
        self.cur.execute('SELECT DISTINCT creator FROM apps')
        creators = [creator[0] for creator in self.cur.fetchall()]
        shuffle(creators)
        self.search_list(creators)

    def run(self):

        self.login(True)

        if self.args.search:
            self.search(self.args.search)

        if self.args.seed_depth:
            self.seed(self.args.seed_depth)

        if self.args.creator:
            self.creator()


def main():

    parser = argparse.ArgumentParser(description = 'Play Store Crawler')
    parser.add_argument('-S', '--seed', action='store', type=int, default=0, dest='seed_depth', help='Do brute force search of depth n')
    parser.add_argument('-T', '--terminate', action='store', type=int, default=0, help='Terminate after n unsuccessful attempts to query new apps')
    parser.add_argument('-C', '--creator', action='store_true', help='Extend database by creator')
    parser.add_argument('-s', '--search', action='store', help='Search for term')
    parser.add_argument('-v', '--verbose', action='store_true', default=False, help='Verbose output')
    parser.add_argument('-c', '--cache', action='store', default=2, type=int, help='How often to query before considering data cached')
    parser.add_argument('-t', '--token', action='store', default="file:///etc/gpcraw/token.dat", help='URL to fetch token from (may be file://)')
    parser.add_argument('-j', '--parallel', action='store', type=int, default=5, help='Start n multiple threads in parallel')
    parser.add_argument('-r', '--refresh', action='store', type=int, default=400, help='Refresh token after 400 seconds')
    parser.add_argument('-d', '--destroy', action='store_true', default=False, help='DANGEROUS! Wipe database.')
    args = parser.parse_args()

    for thread_num in range(1, args.parallel+1):
        # FIXME: Make db name, user and Android ID configurable
        gpc = GPCrawl (
            androidid  = '3d716411bf8bc802',
            dbname     = 'google_play_test',
            user       = 'alex',
            threadid   = 'thread-%2.2d' % (thread_num),
            args       = args)
        gpc.daemon = True
        gpc.start()

    while True: time.sleep(1)

if __name__ == '__main__':
    main()
