#!/usr/bin/env python

import sys
import psycopg2
import datetime
import locale
import string
import time
import argparse

sys.path.append ('googleplay_api')
from googleplay import GooglePlayAPI, LoginError

def enumchar (level, aset):
    if level < 2: return aset
    return [val + newval for newval in aset for val in enumchar(level-1,aset)]

class GPCrawl:

    def __init__(self, tokenfile, androidid, dbname, user, destroy_data = False, max_cached = 1, verbose = False, terminate = 0):

        self.max_cached = max_cached
        self.verbose = verbose
        self.terminate = terminate

        self.num_unsuccessful = 0

        # Set locale such that parsing of download numbers works. Note, that
        # this must be changed consistently with the lang value below.
        locale.setlocale(locale.LC_ALL, 'en_US.utf8')

        token = open(tokenfile, 'r').read()
        self.api = GooglePlayAPI(androidId = androidid, lang = 'en_us')
        self.api.login(None, None, token)

        self.conn = psycopg2.connect('dbname=%s user=%s' % (dbname, user))
        self.cur = self.conn.cursor()

        if destroy_data:
            self.cur.execute ('DROP TABLE apps');

        # Create database:
        #   sudo -u postgres createdb google_play_test
        table = '''
            CREATE TABLE IF NOT EXISTS apps
            (
                search                              TEXT,
                docid                               TEXT,
                title                               TEXT,
                creator                             TEXT,
                offer_micros                        INTEGER,
                offer_currencyCode                  TEXT,
                offer_formatedAmount                TEXT,
                offer_checkoutFlowRequired          BOOLEAN,
                offer_offerType                     INTEGER,
                details_appDetails_versionCode      INTEGER,
                details_appDetails_installationSize INTEGER,
                details_appDetails_numDownloads     BIGINT,
                details_appDetails_packageName      TEXT,
                details_appDetails_uploadDate       TEXT,
                details_appDetails_file_fileType    INTEGER,
                details_appDetails_file_versionCode INTEGER,
                details_appDetails_file_size        BIGINT,
                aggregateRating_type                INTEGER,
                aggregateRating_starRating          REAL,
                aggregateRating_ratingsCount        INTEGER,
                aggregateRating_oneStarRating       INTEGER,
                aggregateRating_twoStarRating       INTEGER,
                aggregateRating_threeStarRating     INTEGER,
                aggregateRating_fourStarRating      INTEGER,
                aggregateRating_fiveStarRating      INTEGER,
                aggregateRating_commentCount        INTEGER,
                detailsUrl                          TEXT,
                sharesUrl                           TEXT,
                detailsReusable                     BOOLEAN,
                duplicates                          INTEGER,
                crawlDate                           TIMESTAMP,
                PRIMARY KEY(details_appDetails_packageName, details_appDetails_versionCode)
            )
        '''
        self.cur.execute(table)

        cache = '''
            CREATE TABLE IF NOT EXISTS cache
            (
                search      TEXT,
                requests    INTEGER,
                PRIMARY KEY(search)
            )
        '''
        self.cur.execute(cache)

        self.conn.commit()

    def mark_cached(self, searchterm):

        querystring = '''
            INSERT INTO cache VALUES (%s,%s)
                ON CONFLICT (search)
                    DO UPDATE SET requests = cache.requests + 1
        '''
        self.cur.execute(querystring, (searchterm,1))
        self.conn.commit()

        if self.verbose:
            print("Marked '%s' as cached" % (searchterm,))

    def cached(self, searchterm):

        self.cur.execute('SELECT requests FROM cache WHERE search LIKE %s', (searchterm,))
        result = self.cur.fetchall()
        if result:
            return result[0][0]

        # Not found
        return 0

    def unique_ids(self):

        self.conn.commit()
        self.cur.execute('SELECT count(*) FROM apps')
        result = self.cur.fetchall()
        return result[0][0]

    def check_terminate(self, new):

        if not self.terminate:
            return

        if (new > 0):
            self.num_unsuccessful = 0
        else:
            self.num_unsuccessful += 1

        if self.num_unsuccessful > self.terminate:
            print ("%d unsuccessful attempts to get new app, terminating" % (self.num_unsuccessful,))
            sys.exit(0)

    def search(self, query):

        num_results = 0
        sleeptime = 30

        num_cached = self.cached (query)
        if  num_cached >= self.max_cached:
            if self.verbose:
                print ("Cached %d times, not performing query for '%s'" % (num_cached, query))
            self.check_terminate(0)
            return 0

        uids_before = self.unique_ids()

        while True:
            try:
                results = self.api.search (query, nb_results=200)
                break
            except IndexError:
                print ("Invalid result, sleeping for %d seconds" % (sleeptime))
                time.sleep (sleeptime)
                sleeptime *= 2

        for docs in results.doc:
            for result in docs.child:

                num_results += 1

                ad = result.details.appDetails
                of = result.offer[0]

                upload_date   = datetime.datetime.strptime(ad.uploadDate, "%b %d, %Y")
                num_downloads = locale.atoi(ad.numDownloads.rstrip('+'))
                querystring = '''
                    INSERT INTO apps VALUES 
                        (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,
                         %s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,now())
                    ON CONFLICT (details_appDetails_packageName, details_appDetails_versionCode)
                        DO UPDATE
                            SET duplicates = apps.duplicates + 1, crawlDate = now()
                '''

                querydata = \
                    (query,
                     result.docid,
                     result.title,
                     result.creator,
                     of.micros,
                     of.currencyCode,
                     of.formattedAmount,
                     of.checkoutFlowRequired,
                     of.offerType,
                     ad.versionCode,
                     ad.installationSize,
                     num_downloads,
                     ad.packageName,
                     upload_date.date(),
                     ad.file[0].fileType,
                     ad.file[0].versionCode,
                     ad.file[0].size,
                     result.aggregateRating.type,
                     result.aggregateRating.starRating,
                     result.aggregateRating.ratingsCount,
                     result.aggregateRating.oneStarRatings,
                     result.aggregateRating.twoStarRatings,
                     result.aggregateRating.threeStarRatings,
                     result.aggregateRating.fourStarRatings,
                     result.aggregateRating.fiveStarRatings,
                     result.aggregateRating.commentCount,
                     result.detailsUrl,
                     result.shareUrl,
                     result.detailsReusable,
                     0)

                try:
                    self.cur.execute(querystring, querydata)
                except psycopg2.DataError as e:
                    print ("Data error: %s" % (str(e),))
                    print (str(querydata))

        self.conn.commit()
        self.mark_cached(query)

        uids_delta = self.unique_ids() - uids_before
        self.check_terminate(uids_delta)

        if self.verbose:
            print ("Searched '%s', got %d results, %d new packages" % (query, num_results, uids_delta))

        return uids_delta

    def seed (self, length):
        for run in (1,2):
            for query in enumchar(length, string.ascii_lowercase):
                self.search(query)

    def creator(self):
        self.cur.execute('SELECT details_appdetails_packagename, creator FROM apps')
        for (package, creator) in self.cur.fetchall():
            self.search(creator)

def main():

    parser = argparse.ArgumentParser(description = 'Play Store Crawler')
    parser.add_argument('-S', '--seed', action='store', type=int, default=0, dest='seed_depth', help='Do brute force search of depth n')
    parser.add_argument('-T', '--terminate', action='store', type=int, default=0, help='Terminate after n unsuccessful attempts to query new apps')
    parser.add_argument('-C', '--creator', action='store_true', help='Extend database by creator')
    parser.add_argument('-s', '--search', action='store', help='Search for term')
    parser.add_argument('-v', '--verbose', action='store_true', default=False, help='Verbose output')
    parser.add_argument('-c', '--cache', action='store', default=2, help='How often to query before considering data cached')
    args = parser.parse_args()

    # FIXME: Make db name, user, token file and Android ID configurable
    gpc = GPCrawl (tokenfile = 'token.dat', androidid = '3d716411bf8bc802', dbname = 'google_play_test', user = 'alex', verbose = args.verbose, max_cached = args.cache, terminate = args.terminate)

    if args.seed_depth:
        gpc.seed(args.seed_depth)

    if args.search:
        gpc.search(args.search)

    if args.creator:
        gpc.creator()

if __name__ == '__main__':
    main()
