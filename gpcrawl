#!/usr/bin/env python
import sys
import psycopg2
import datetime
import locale
import string
import time
import argparse
import urllib2
import threading
import os
from random import shuffle
from datetime import timedelta

from stem import Signal
from stem.control import Controller

sys.path.append ('googleplay_api')
from googleplay import GooglePlayAPI, LoginError

def enumchar (level, aset):
    if level < 2: return aset
    return [val + newval for newval in aset for val in enumchar(level-1,aset)]

class Terminate(Exception): pass

failed_searches = 0
num_searches = 0
update_failed = threading.Lock()

class GPAPI:

    def __init__(self, androidid, tokenurl, delay = 0, verbose = 0):

        self.androidid = androidid
        self.tokenurl  = tokenurl
        self.verbose   = verbose
        self.delay     = delay

        self.controller = Controller.from_socket_file()
        self.controller.authenticate()

        if self.verbose:
            print ("[gpapi] Tor version %s" % self.controller.get_version())

        # Create new circuits
        newnym_delay = self.controller.get_newnym_wait()
        if self.verbose:
            print ("[gpapi] Waiting %d sec until NEWNYM" % newnym_delay)
        time.sleep(newnym_delay + 5)
        self.controller.signal(Signal.NEWNYM)

        # Set locale such that parsing of download numbers works. Note, that
        # this must be changed consistently with the lang value below.
        locale.setlocale(locale.LC_ALL, 'en_US.utf8')

        self.__api = GooglePlayAPI(androidId = self.androidid, lang = 'en_us')

        if self.verbose:
            print ("[gpapi] Fetching token from '%s'" % (self.tokenurl,))
        self.token = urllib2.urlopen(self.tokenurl).read()
        if self.verbose:
            print ("[gpapi] Logging in with token '%s'" % (self.token))
        self.__api.login(None, None, self.token)

    def update_failed_searches(self, value):

        global update_failed
        global failed_searches
        global num_searches
        update_failed.acquire()
        failed_searches += value
        num_searches += 1
        update_failed.release()

    def search(self, query):

        while True:

            num_errors = 0
            try:
                result = self.__api.search (query, nb_results=200)
                self.update_failed_searches(-1)
                break
            except IndexError:
                self.update_failed_searches(1)
                num_errors += 1
                if num_errors > 2:
                    os._exit(0)
            except Exception as e:
                print ("[gpapi] Search exception: '%s'" % (str(e)))
                return None

        return result

class GPDB:

    def __init__(self, dbname, user, destroy):

        self.conn = psycopg2.connect('dbname=%s user=%s' % (dbname, user))
        self.cur = self.conn.cursor()

        if destroy:
            self.cur.execute ('DROP TABLE apps');

        # Create database:
        #   sudo -u postgres createdb google_play_test
        table = '''
            CREATE TABLE IF NOT EXISTS apps
            (
                search                              TEXT,
                docid                               TEXT,
                title                               TEXT,
                creator                             TEXT,
                offer_micros                        BIGINT,
                offer_currencyCode                  TEXT,
                offer_formatedAmount                TEXT,
                offer_checkoutFlowRequired          BOOLEAN,
                offer_offerType                     INTEGER,
                details_appDetails_versionCode      INTEGER,
                details_appDetails_installationSize BIGINT,
                details_appDetails_numDownloads     BIGINT,
                details_appDetails_packageName      TEXT,
                details_appDetails_uploadDate       TEXT,
                details_appDetails_file_fileType    INTEGER,
                details_appDetails_file_versionCode INTEGER,
                details_appDetails_file_size        BIGINT,
                aggregateRating_type                INTEGER,
                aggregateRating_starRating          REAL,
                aggregateRating_ratingsCount        INTEGER,
                aggregateRating_oneStarRating       INTEGER,
                aggregateRating_twoStarRating       INTEGER,
                aggregateRating_threeStarRating     INTEGER,
                aggregateRating_fourStarRating      INTEGER,
                aggregateRating_fiveStarRating      INTEGER,
                aggregateRating_commentCount        INTEGER,
                detailsUrl                          TEXT,
                sharesUrl                           TEXT,
                detailsReusable                     BOOLEAN,
                duplicates                          INTEGER,
                crawlDate                           TIMESTAMP,
                PRIMARY KEY(details_appDetails_packageName, details_appDetails_versionCode)
            )
        '''
        self.cur.execute(table)

        cache = '''
            CREATE TABLE IF NOT EXISTS cache
            (
                search      TEXT,
                requests    INTEGER,
                PRIMARY KEY(search)
            )
        '''
        self.cur.execute(cache)
        self.conn.commit()

    def unique_ids(self):

        self.conn.commit()
        self.cur.execute('SELECT count(*) FROM apps')
        result = self.cur.fetchall()
        return result[0][0]

class Statistics (threading.Thread, GPDB):

    def __init__(self, dbname, user, failed_max, num_threads):

        # Initialize threading
        threading.Thread.__init__(self)
        GPDB.__init__(self, dbname, user, False)
        self.__failed_max = failed_max
        self.__num_threads = num_threads

    def run(self):

        global failed_searches
        global num_searches

        start_time = time.time()
        start_ids  = self.unique_ids()

        old = start_ids
        while True:
            unique_ids   = self.unique_ids()
            current_time = time.time()
            rate_since_start = 60 * (unique_ids - start_ids) / (current_time - start_time)

            print ("[statistics] %d unique applications, %d per minute, %d per minute average, %d/%d failed." % (unique_ids, 4 * (unique_ids - old), rate_since_start, failed_searches, num_searches))

            if self.__failed_max > 0 and failed_searches > self.__failed_max:
                print ("[result] %d apps, %d average, %d threads, %d searches" % (unique_ids, rate_since_start, self.__num_threads, num_searches))
                print ("Exiting...")
                os._exit(0)
                return

            time.sleep(15)
            old = unique_ids

class GPCrawl (threading.Thread, GPDB):

    def __init__(self, api, dbname, user, args, threadid):

        # Initialize threading
        threading.Thread.__init__(self)
        GPDB.__init__(self, dbname, user, args.destroy)

        self.threadid = threadid
        self.args = args
        self.verbose = args.verbose

        self.num_unsuccessful = 0

        self.api = api

    def mark_cached(self, searchterm):

        querystring = '''
            INSERT INTO cache VALUES (%s,%s)
                ON CONFLICT (search)
                    DO UPDATE SET requests = cache.requests + 1
        '''
        self.cur.execute(querystring, (searchterm,1))
        self.conn.commit()

        if self.verbose > 1:
            print("[%s]: ........ Marked '%s' as cached" % (self.threadid, searchterm))

    def cached(self, searchterm):

        self.cur.execute('SELECT requests FROM cache WHERE search LIKE %s', (searchterm,))
        result = self.cur.fetchall()
        if result:
            return result[0][0]

        # Not found
        return 0

    def search(self, query):

        num_fail = 0
        num_results = 0
        sleeptime = 30

        num_cached = self.cached (query)
        if  num_cached >= self.args.cache:
            if self.verbose > 1:
                print ("[%s]: ........ Cached %d times, not performing query for '%s'" % (self.threadid, num_cached, query))
            return 0

        uids_before = self.unique_ids()

        num_errors = 0
        results = self.api.search (query)

        for docs in results.doc:
            for result in docs.child:

                num_results += 1

                ad = result.details.appDetails
                of = result.offer[0]

                upload_date   = datetime.datetime.strptime(ad.uploadDate, "%b %d, %Y")
                num_downloads = locale.atoi(ad.numDownloads.rstrip('+'))
                querystring = '''
                    INSERT INTO apps VALUES
                        (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,
                         %s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,now())
                    ON CONFLICT (details_appDetails_packageName, details_appDetails_versionCode)
                        DO UPDATE
                            SET duplicates = apps.duplicates + 1, crawlDate = now()
                '''

                querydata = \
                    (query,
                     result.docid,
                     result.title,
                     result.creator,
                     of.micros,
                     of.currencyCode,
                     of.formattedAmount,
                     of.checkoutFlowRequired,
                     of.offerType,
                     ad.versionCode,
                     ad.installationSize,
                     num_downloads,
                     ad.packageName,
                     upload_date.date(),
                     ad.file[0].fileType,
                     ad.file[0].versionCode,
                     ad.file[0].size,
                     result.aggregateRating.type,
                     result.aggregateRating.starRating,
                     result.aggregateRating.ratingsCount,
                     result.aggregateRating.oneStarRatings,
                     result.aggregateRating.twoStarRatings,
                     result.aggregateRating.threeStarRatings,
                     result.aggregateRating.fourStarRatings,
                     result.aggregateRating.fiveStarRatings,
                     result.aggregateRating.commentCount,
                     result.detailsUrl,
                     result.shareUrl,
                     result.detailsReusable,
                     0)

                try:
                    self.cur.execute(querystring, querydata)
                except psycopg2.DataError as e:
                    print ("[%s]: ........ Data error: %s" % (self.threadid, str(e)))
                    print ("[%s]: ........ %s" % (self.threadid, str(querydata)))

        self.conn.commit()
        self.mark_cached(query)

        if self.verbose:
            print ("[%s]: ........ Got %3.0d packages, query '%s'" % (self.threadid, num_results, query))

    def search_list(self, queries):

        query_num = 0
        for query in queries:
            self.search(query)
            query_num += 1
            if self.verbose > 1:
                print ("[%s]: ........ Searching %d/%d: '%s'" % (self.threadid, query_num, len(queries), query))
            self.search(query)

    def seed(self, length):
        for run in (1,2):
            self.search_list(enumchar(length, string.ascii_lowercase))

    def extend_creator(self):
        self.cur.execute('SELECT DISTINCT creator FROM apps WHERE random() < 0.001')
        creators = [creator[0] for creator in self.cur.fetchall()]
        shuffle(creators)
        self.search_list(creators)

    def tokens(self):
        tokens = {}
        self.cur.execute('SELECT title FROM apps')
        result = self.cur.fetchall()
        for (title,) in result:
            for t in title.split():
                t = t.lower()
                if t in tokens:
                    tokens[t] += 1
                else:
                    tokens[t] = 1

        return tokens

    def extend_tokens(self):
        tokens = self.tokens().keys()
        shuffle(tokens)
        self.search_list(tokens)

    def packages(self):
        names = {}
        self.cur.execute('SELECT details_appdetails_packagename FROM apps')
        result = self.cur.fetchall()
        for (pname,) in result:
            plist = pname.split('.')
            for i in range(0, len(plist)):  # Ignore last component as this is the known package!
                psubname = '.'.join(plist[0:i])
                if psubname in names:
                    names[psubname] += 1
                else:
                    names[psubname] = 1

        return names

    def extend_packages(self):
        packages = self.packages().keys()
        shuffle(packages)
        self.search_list(packages)

    def run(self):

        try:

            if self.args.search:
                self.search(self.args.search)

            if self.args.seed_depth:
                self.seed(self.args.seed_depth)

            if self.args.creator:
                self.extend_creator()

            if self.args.packages:
                self.extend_packages()

            if self.args.titles:
                self.extend_titles()

        except Terminate:
            pass

def sledgehammer(args):

    api = GPAPI(androidid  = args.androidid, tokenurl = args.token, verbose = args.verbose)

    threads = []

    for thread_num in range(1, args.parallel+1):
        thread_name = 'thread-%2.2d' % (thread_num)
        print ("[main]: Starting %s" % (thread_name,))
        gpc = GPCrawl (
            dbname     = args.dbname,
            user       = args.dbuser,
            threadid   = thread_name,
            api        = api,
            args       = args)
        gpc.daemon = True
        threads.append (gpc)
        gpc.start()

    for thread in threads:
        thread.join()

def main():

    parser = argparse.ArgumentParser(description = 'Play Store Crawler')
    parser.add_argument('-S', '--seed', action='store', type=int, default=0, dest='seed_depth', help='Do brute force search of depth n')
    parser.add_argument('-X', '--terminate', action='store', type=int, default=0, help='Terminate after n unsuccessful attempts to query new apps')
    parser.add_argument('-C', '--creator', action='store_true', help='Extend database by creator')
    parser.add_argument('-P', '--packages', action='store_true', help='Extend database by package name')
    parser.add_argument('-T', '--titles', action='store_true', help='Extend database by titles')
    parser.add_argument('-D', '--delay', action='store', type=int, default=10, help='Delay between queries (in seconds)')
    parser.add_argument('-s', '--search', action='store', help='Search for term')
    parser.add_argument('-v', '--verbose', action='count', help='Verbose output')
    parser.add_argument('-c', '--cache', action='store', default=2, type=int, help='How often to query before considering data cached')
    parser.add_argument('-t', '--token', action='store', default="file:///etc/gpcrawl/token.dat", help='URL to fetch token from (may be file://)')
    parser.add_argument('-j', '--parallel', action='store', type=int, default=1, help='Start n multiple threads in parallel')
    parser.add_argument('-x', '--destroy', action='store_true', default=False, help='DANGEROUS! Wipe database.')
    parser.add_argument('-d', '--dbuser', action='store', help='Database user to use.')
    parser.add_argument('-n', '--dbname', action='store', help='Database name to use.')
    parser.add_argument('-a', '--androidid', action='store', help='Android ID to use.')
    args = parser.parse_args()

    stats = Statistics(dbname = args.dbname, user = args.dbuser, failed_max = args.terminate, num_threads = args.parallel)
    stats.daemon = True
    stats.start()

    while True:
        try:
            sledgehammer(args)
        except KeyboardInterrupt: raise
        except: pass
        time.sleep (60)

if __name__ == '__main__':
    main()
